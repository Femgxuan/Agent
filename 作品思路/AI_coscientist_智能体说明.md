### 各智能体功能解读及设计可能性分析


#### 1. **user_agent（用户代理）**  
- **功能**：与用户交互，将用户的研究目标提炼为包含“目标、偏好、属性、约束”的结构化配置（JSON格式），明确研究核心需求。  
- **设计可能性**：用户输入通常是自然语言描述，可能模糊或不规范。该智能体通过标准化处理，将需求转化为系统可理解的参数，为后续智能体的协作提供统一“任务说明书”，避免信息偏差。


#### 2. **generator_init（初始假设生成器）**  
- **功能**：作为领域专家，调用工具获取最新文献，基于用户需求和文献生成多个具体、可验证的初始假设，需包含科学背景、研究进展和创新突破。  
- **设计可能性**：科学假设需基于现有研究，因此绑定文献检索工具（如PubMed）确保时效性；同时要求假设“具体、可验证”，避免空泛，为后续辩论和优化奠定基础。


#### 3. **reviewer1-5（辩论者）**  
- **功能**：参与多轮协作讨论，初始提出3个新假设，后续通过质疑、评价（如新颖性、可行性）、改进现有假设，最终输出精炼后的假设。  
- **设计可能性**：模拟科学研究中的“同行评议”机制，多专家视角可减少偏见，通过辩论暴露假设的漏洞（如逻辑缺陷、可行性低），推动假设迭代至更严谨状态。


#### 4. **debate_supervisor（辩论主持人）**  
- **功能**：协调5名辩论者的讨论流程，确保每人至少参与1次，控制讨论轮次（5-8轮，最多10轮），最终综合所有观点生成统一的最终假设。  
- **设计可能性**：避免讨论混乱或某一视角主导，通过平衡参与度和轮次，确保讨论全面且高效，最终整合分歧形成共识性假设。


#### 5. **generation_format（格式整合器）**  
- **功能**：从不同来源（初始假设、辩论生成假设、迭代假设等）提取假设内容，合并为有序列表，不修改原意。  
- **设计可能性**：多智能体生成的假设格式可能不一致（如不同编号、表述风格），该智能体通过标准化整合，便于后续排序、评审等步骤高效进行。


#### 6. **reviewer1-5（辩论者）**  
- **功能**：参与多轮协作讨论，初始提出3个新假设，后续通过质疑、评价（如新颖性、可行性）、改进现有假设，最终输出精炼后的假设。  
- **设计可能性**：模拟科学研究中的“同行评议”机制，多专家视角可减少偏见，通过辩论暴露假设的漏洞（如逻辑缺陷、可行性低），推动假设迭代至更严谨状态。


#### 7. **evolution1-6（假设进化器）**  
- **功能**：从6个维度优化假设，包括：  
  - 增强逻辑严谨性（Enhancement）、提升可行性（Feasibility）、组合优质假设（Combination）、简化验证流程（Simplification）、基于现有假设生成新灵感（Inspiration）、跳出传统框架的发散思维（Out-of-the-box）。  
- **设计可能性**：单一假设可能存在多方面缺陷（如逻辑不完整、实验难执行），多维度优化可针对性解决不同问题，覆盖从“修补漏洞”到“突破创新”的全需求。


#### 8. **ranker（假设排序器）**  
- **功能**：调用“elo_tournament”工具对假设进行排序，无需额外参数。  
- **设计可能性**：生成的假设数量可能较多，需通过排序突出优质选项。ELO系统常用于竞技排名，适合多假设的相对质量比较，帮助后续流程聚焦高潜力假设。


#### 9. **generate_meta_review（元评审生成器）**  
- **功能**：将排序后的假设转化为完整研究报告，包括背景、核心问题、实验设计、未来方向等，输出为结构化JSON。  
- **设计可能性**：假设需转化为可操作的研究计划才能落地。该智能体填补“理论假设”与“实验执行”的鸿沟，为研究者提供具体指导（如实验步骤、数据分析方法）。


#### 10. **reflection相关智能体（反思评审器）**  
- **功能**：从多个维度评估假设，包括：  
  - 初步评审（逻辑一致性、安全性）、深度验证（拆解假设为子假设并检验正确性）、模拟评审（预测实验失败场景）、文献匹配度（与现有研究的契合度）等。  
- **设计可能性**：科学假设需经过严格验证才能避免无效研究。多维度评审可过滤掉逻辑错误、缺乏新颖性或伦理风险的假设，确保留存假设的科学性和可靠性。


#### 11. **iteration_generator（迭代假设生成器）**  
- **功能**：基于用户需求、文献和前序评审，拆解假设为可测试的一级假设和子假设，形成逻辑链，最终生成新假设。  
- **设计可能性**：复杂假设需拆解为更简单的“可验证单元”（如子假设），才能逐步验证其合理性。该智能体通过逻辑拆解，降低验证难度，构建严谨的理论基础。


#### 12. **review_generator（新方向生成器）**  
- **功能**：分析现有假设的盲区，提出2-4个未被充分探索的研究方向，并基于方向生成新假设。  
- **设计可能性**：避免研究局限于现有框架，通过挖掘“空白领域”（如未被关注的机制、跨学科角度）推动创新，模拟科学研究中“从已知到未知”的探索过程。


### 总结  
各智能体分工明确，覆盖从“用户需求解析”到“研究计划落地”的全流程，模拟了科学研究中“提出问题→生成假设→同行评议→优化验证→落地执行”的协作逻辑。通过多角色协作、多维度优化和严格评审，最终提升假设的科学性、创新性和可行性。



# elo_tournament


假设A的初始分数为1000分，B的初始分数为1200分，A在与B的对决中获胜，k值为16，计算A和B的新分数。

根据ELO评分公式，计算过程如下：


### 步骤1：计算A和B的预期胜率  
预期胜率公式：  
`预期胜率A = 1 / (1 + 10^((B分数 - A分数)/400))`  
`预期胜率B = 1 - 预期胜率A`（因A和B的预期胜率总和为1）  


代入数据（A=1000分，B=1200分）：  
- B与A的分数差：1200 - 1000 = 200分  
- 预期胜率A = 1 / (1 + 10^(200/400)) = 1 / (1 + 10^0.5) ≈ 1 / (1 + 3.162) ≈ 1 / 4.162 ≈ **0.2403**（24.03%）  
- 预期胜率B = 1 - 0.2403 = **0.7597**（75.97%）  


### 步骤2：根据实际结果计算新分数  
实际结果：A胜（A得1分，B得0分），k=16  

新分数公式：  
`新分数 = 旧分数 + k × (实际结果 - 预期胜率)`  


- A的新分数：  
1000 + 16 × (1 - 0.2403) ≈ 1000 + 16 × 0.7597 ≈ 1000 + 12.155 ≈ **1012.16分**（保留两位小数）  

- B的新分数：  
1200 + 16 × (0 - 0.7597) ≈ 1200 + 16 × (-0.7597) ≈ 1200 - 12.155 ≈ **1187.84分**（保留两位小数）  


### 最终结果  
- A的新分数：**1012.16分**  
- B的新分数：**1187.84分**  

（注：A作为低分段爆冷获胜，分数提升明显；B作为高分段意外失利，分数下降幅度相同，符合ELO“强者失利扣分多、弱者爆冷加分多”的逻辑。）




### （1）配对机制：如何选择“合适的对手”进行对比？  
配对的核心目标是**让有意义的对比更频繁发生**（避免强弱悬殊的无效PK，同时让优质假设多参与对比），具体分三步实现：  


#### ① 筛选“分数接近”的候选对手  
系统会先过滤出ELO分数差小于`elo_threshold`（如200分）的假设对。  
- **为什么？** 若A（1500分）和B（1000分）分数差过大（500分），A几乎肯定会赢，这种对比对双方分数调整的意义很小（无法有效区分质量）。而分数接近的假设（如1250分和1300分），胜负不确定性更高，对比结果能更精准反映质量差异。  
- **操作**：遍历所有假设，计算任意两个假设的分数差，仅保留差≤200分的对作为候选。  


#### ② 按“ELO总和概率”选择最终配对  
从候选对中，按“两个假设的ELO分数总和”分配被选中的概率（总和越高，被选中的概率越大）。  
- **为什么？** 高分假设（如1300分以上）更可能是优质假设，需要更多对比来确定其在排名中的准确位置（避免“高分但少对比”导致的排名偏差）。  
- **举例**：候选对有两组——（A:1200分，B:1250分，总和2450）和（C:1100分，D:1150分，总和2250）。  
  总和占比：2450/(2450+2250)≈52.1%，2250/(2450+2250)≈47.9%。  
  因此，（A,B）被选中的概率更高（约52.1%），会优先参与本轮对比。  


#### ③ 控制每轮对比数量（max_matches）  
每轮最多选择`max_matches`对（如5对）进行对比。  
- **为什么？** 若假设数量多（如20个），两两对比的组合数会非常大（C(20,2)=190对），全部对比会消耗大量计算资源（LLM调用成本高）。限制数量可在“排名准确性”和“效率”之间平衡。  


### （2）胜负判断机制：如何通过LLM评估假设优劣？  
胜负判断的核心是**模拟“专家评审”**，但根据假设的当前排名（ELO分数）采用不同严格程度的评估方式，确保高潜力假设经过更严谨的检验：  


#### ① 低排名假设（ELO≤1250）：单轮快速评估（ranking_prompt_single_turn）  
- **适用场景**：这类假设可能处于“初步筛选”阶段，质量尚未明确，无需过度消耗资源，快速淘汰明显劣质的即可。  
- **评估流程**：  
  - 给LLM输入预设提示词模板（`ranking_prompt_single_turn`），模板中明确评估标准（如“逻辑严谨性”“与研究目标的相关性”“实验可行性”等）。  
  - 同时输入两个假设的内容，要求LLM直接判断“哪个更优”，并给出简短理由（如“假设A的逻辑存在漏洞，假设B更可行”）。  
  - LLM输出结果即为胜负（A胜/B胜/平局）。  


#### ② 高排名假设（ELO>1250）：多轮辩论式评估（ranking_prompt_debate）  
- **适用场景**：这类假设已通过初步筛选，可能是最终排名靠前的优质假设，需要更严格的检验来避免“误判”。  
- **评估流程**：  
  - 给LLM输入辩论式提示词模板（`ranking_prompt_debate`），要求模拟“两位专家”对两个假设进行多轮攻防。  
  - 流程示例：  
    1. 第一轮：专家1为假设A辩护，指出其优势；专家2为假设B辩护，指出其优势。  
    2. 第二轮：专家1质疑假设B的漏洞，专家2回应；同时专家2质疑假设A的漏洞，专家1回应。  
    3. 第三轮：双方总结，强调各自假设的核心优势。  
  - 最后，LLM作为“裁判”综合辩论过程，判断哪个假设更优（逻辑更严谨、创新度更高、可行性更强等）。  


### 总结  
配对机制通过“分数接近+高分优先+数量控制”确保对比的有效性和效率；胜负判断通过“分层评估”（低排名快速筛、高排名严格辩）平衡资源消耗与评估准确性，最终让ELO分数更真实反映假设质量。